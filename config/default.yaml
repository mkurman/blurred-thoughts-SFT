# Model configuration
model_name: "llama"
threshold: 0.2
bf_beta: 0.05

# Checkpoints
checkpoint: "mkurman/Llama-3.2-MedIT-SUN-2.5B-BT-GRPO"
tokenizer_name: "mkurman/Llama-3.2-MedIT-SUN-2.5B-BT-GRPO"
trainer_checkpoint: null
base_model_checkpoint: null

# Dataset
dataset_train: "mkurman/simplescaling-s1K-R1"
max_length: 512

# Training parameters
save_steps: 500
batch_size: 32
accumulation_iter: 12
epochs: 1
lr: 5e-5
warmup_steps: 500
weight_decay: 0.01

# Directories
logging_dir: "./logs"
output_dir: "./results"
cache_dir: null

# Data handling
train_test_split: 0.1
seed: 42
skip: 0
take: null
num_workers: 24

# Other settings
device: "cuda"
response_template: "<|start_header_id|>assistant<|end_header_id|>\n\n"
lora_rank: 64